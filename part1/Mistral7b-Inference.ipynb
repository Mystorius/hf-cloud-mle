{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvD7X2HYUZ_L"
      },
      "outputs": [],
      "source": [
        "!pip install transformers optimum auto-gptq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EP7o7yI8W3L4"
      },
      "outputs": [],
      "source": [
        "# You should login to the ðŸ¤—Hub to download the model\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_OxLyQLWOo9"
      },
      "outputs": [],
      "source": [
        "# The easiest way is to load the model via ðŸ¤— Transformer - Pipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful Lead Machine Learning Engineer that gets user started with Mistral7B\"},\n",
        "    {\"role\": \"user\", \"content\": \"How can I run Mistral7b on the free tier version of Google Colab?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\", device_map=\"auto\", max_new_tokens=256)\n",
        "pipe(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy_OQGRmhvFQ"
      },
      "outputs": [],
      "source": [
        "# For faster inference, you can also use an optimised model from the ðŸ¤— Community\n",
        "\n",
        "# Import necessary modules from Hugging Face transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Define the model path from Hugging Face Hub\n",
        "model_name_or_path = \"thesven/Mistral-7B-Instruct-v0.3-GPTQ\"\n",
        "\n",
        "# Load the tokenizer with fast tokenization\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "# Load the model with automatic device mapping\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "# Set the padding token to the end-of-sequence token\n",
        "model.pad_token = model.config.eos_token_id\n",
        "\n",
        "# Define the prompt template with system and instruction tags\n",
        "prompt_template = '''\n",
        "<s><<SYS>>You are a helpful Lead Machine Learning Engineer that gets user started with Mistral7B:</s><</SYS>>\n",
        "<s>[INST]How can I run Mistral7b on the free tier version of Google Colab</s>[/INST]\n",
        "<s>[ASSISTANT]\n",
        "'''\n",
        "\n",
        "# Tokenize the prompt and move input IDs to the GPU\n",
        "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "\n",
        "# Generate a response from the model with specified decoding parameters\n",
        "output = model.generate(inputs=input_ids,\n",
        "                        temperature=0.1,\n",
        "                        do_sample=True,\n",
        "                        top_p=0.95,\n",
        "                        top_k=40,\n",
        "                        max_new_tokens=256)\n",
        "\n",
        "# Decode and print the generated output\n",
        "print(tokenizer.decode(output[0]))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
